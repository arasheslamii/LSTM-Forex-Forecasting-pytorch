{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2wE26o4NVMb",
        "outputId": "195efc0e-4999-4918-8203-dcbc39038511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Libraries and Framework importation\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "end  = pd.to_datetime( datetime.now())\n",
        "start = end  - timedelta(days= 59)\n",
        "data = yf.download('USDJPY=X', start = start , end = end, interval= '5m' )\n",
        "df = torch.tensor(data.drop(columns=['Adj Close', 'Volume']).reset_index().drop(columns = ['Datetime']).to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y =  [], []\n",
        "for i in range(0,len(df)-144-60):\n",
        "  X.append(df[i:i+144])\n",
        "  y.append(df[i+144:(i+144+60)])"
      ],
      "metadata": {
        "id": "OfEIzwIhRXF1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating X_train, X_test, y_train, y_test\n",
        "\n",
        "train_size = 0.8\n",
        "X_train  = np.array(X[:int(train_size*len(X))])\n",
        "X_test  = np.array(X[int(train_size*len(X)):])\n",
        "y_train  = np.array(y[:int(train_size*len(y))])\n",
        "y_test  = np.array(y[int(train_size*len(y)):])\n",
        "\n",
        "#reshape to get ready for minmax scaling, because minmax scale got only 2D array\n",
        "\n",
        "X_train_reshape  = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_reshape  = X_test.reshape(-1, X_test.shape[-1])\n",
        "y_train_reshape  = y_train.reshape(-1, y_train.shape[-1])\n",
        "y_test_reshape  = y_test.reshape(-1, y_train.shape[-1])\n",
        "\n",
        "#Scaling the data from #_###_reshape to\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "X_train_sc = scaler.fit_transform(X_train_reshape)\n",
        "y_train_sc = scaler.fit_transform(y_train_reshape)\n",
        "X_test_sc = scaler.transform(X_test_reshape)\n",
        "y_test_sc = scaler.transform(y_test_reshape )\n",
        "\n",
        "# Reshape the scaled data to getting back to their original shape\n",
        "X_train_sc = torch.from_numpy( X_train_sc.reshape(X_train.shape)).float()\n",
        "y_train_sc = torch.from_numpy(y_train_sc .reshape(y_train.shape)).float()\n",
        "X_test_sc = torch.from_numpy(X_test_sc .reshape(X_test.shape)).float()\n",
        "y_test_sc = torch.from_numpy(y_test_sc .reshape(y_test.shape)).float()"
      ],
      "metadata": {
        "id": "qkBKZaQiTCKc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'shape of the X_train: {X_train_sc.shape}, shpe of th X_test: {X_test.shape}, shpe of th y_train: {y_train.shape}, shpe of th y_test: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eau0WoeCg9Dx",
        "outputId": "7581459a-3dae-4b05-9976-f4856fbaa258"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the X_train: torch.Size([9412, 144, 4]), shpe of th X_test: (2353, 144, 4), shpe of th y_train: (9412, 60, 4), shpe of th y_test: (2353, 60, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EDK0f9BYTf8F"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self, X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  def  __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataloader = DataLoader(custom_dataset(\n",
        "    X_train_sc, y_train_sc), batch_size = 32)\n",
        "\n",
        "test_dataloader = DataLoader(custom_dataset(\n",
        "    X_test_sc, y_test_sc), batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g_DuaNsabwh"
      },
      "source": [
        "Lets Creeat a custome Dataset to make a dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zQqWOtiwQJDZ"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size , hidden_size , num_layers, output_size , dropout_prob):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.lstm  = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                         num_layers=num_layers, batch_first=True,\n",
        "                         dropout=dropout_prob)\n",
        "    self.fc1 = nn.Linear(hidden_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc_out = nn.Linear(64, output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "  def forward(self, X):  # This should be outside __init__\n",
        "    lstm_out, _ = self.lstm(X)\n",
        "    lstm_out = lstm_out[:, -1, :]  # Only keep the output from the last time step\n",
        "    out = self.relu(self.fc1(lstm_out))\n",
        "    out = self.dropout(out)\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc_out(out)\n",
        "    return out.view(out.size(0), 60, 4)\n",
        "\n",
        "input_size = 4  # Number of input features (Open, High, Low, Close)\n",
        "hidden_size = 128  # Number of LSTM neurons in each layer\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "output_size = 240  # Predicting 60 future time steps, 4 features for each (60 * 4 = 240)\n",
        "dropout_prob = 0.3  # 30% dropout rate\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
        "\n",
        "# Initialize the loss function adn Optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "#Lets define our crucial things\n",
        "learning_rate = 1e-5\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JxIMmZXjaDPy"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss = loss.item()\n",
        "      current = batch * batch_size +len(X)\n",
        "      print(f\"loss \\U0001F432: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Testing/Validation loop\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients during testing\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()  # Accumulate the loss\n",
        "\n",
        "    test_loss /= num_batches  # Average loss over all batches\n",
        "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "abPth-GO6qLv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS7xXxA5Nids",
        "outputId": "521a50d5-4b28-4c01-e8de-5a6f4c4756f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.689735  [   32/ 9422]\n",
            "loss 🐲: 0.200350  [ 3232/ 9422]\n",
            "loss 🐲: 0.098525  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.440978 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.672932  [   32/ 9422]\n",
            "loss 🐲: 0.183608  [ 3232/ 9422]\n",
            "loss 🐲: 0.071424  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.278003 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.438334  [   32/ 9422]\n",
            "loss 🐲: 0.086145  [ 3232/ 9422]\n",
            "loss 🐲: 0.031896  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.167306 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.285459  [   32/ 9422]\n",
            "loss 🐲: 0.062696  [ 3232/ 9422]\n",
            "loss 🐲: 0.019347  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.112722 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.201754  [   32/ 9422]\n",
            "loss 🐲: 0.044761  [ 3232/ 9422]\n",
            "loss 🐲: 0.012041  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.078578 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.142762  [   32/ 9422]\n",
            "loss 🐲: 0.028818  [ 3232/ 9422]\n",
            "loss 🐲: 0.008582  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.054974 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.102017  [   32/ 9422]\n",
            "loss 🐲: 0.018547  [ 3232/ 9422]\n",
            "loss 🐲: 0.007284  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.039159 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.079676  [   32/ 9422]\n",
            "loss 🐲: 0.012694  [ 3232/ 9422]\n",
            "loss 🐲: 0.006318  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.028541 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.062622  [   32/ 9422]\n",
            "loss 🐲: 0.009455  [ 3232/ 9422]\n",
            "loss 🐲: 0.005957  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.021349 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.052980  [   32/ 9422]\n",
            "loss 🐲: 0.007983  [ 3232/ 9422]\n",
            "loss 🐲: 0.005116  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.016616 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.041782  [   32/ 9422]\n",
            "loss 🐲: 0.007070  [ 3232/ 9422]\n",
            "loss 🐲: 0.004942  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.013662 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.034757  [   32/ 9422]\n",
            "loss 🐲: 0.006560  [ 3232/ 9422]\n",
            "loss 🐲: 0.004375  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.011610 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.036758  [   32/ 9422]\n",
            "loss 🐲: 0.005433  [ 3232/ 9422]\n",
            "loss 🐲: 0.004172  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.009993 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.027986  [   32/ 9422]\n",
            "loss 🐲: 0.005570  [ 3232/ 9422]\n",
            "loss 🐲: 0.003769  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.009022 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.027796  [   32/ 9422]\n",
            "loss 🐲: 0.005659  [ 3232/ 9422]\n",
            "loss 🐲: 0.003590  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.008405 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.022589  [   32/ 9422]\n",
            "loss 🐲: 0.004714  [ 3232/ 9422]\n",
            "loss 🐲: 0.003674  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007657 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.023753  [   32/ 9422]\n",
            "loss 🐲: 0.004642  [ 3232/ 9422]\n",
            "loss 🐲: 0.003589  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007784 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.023998  [   32/ 9422]\n",
            "loss 🐲: 0.004801  [ 3232/ 9422]\n",
            "loss 🐲: 0.003466  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007152 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.021440  [   32/ 9422]\n",
            "loss 🐲: 0.005155  [ 3232/ 9422]\n",
            "loss 🐲: 0.003334  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007002 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.023364  [   32/ 9422]\n",
            "loss 🐲: 0.004339  [ 3232/ 9422]\n",
            "loss 🐲: 0.003295  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006827 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.023954  [   32/ 9422]\n",
            "loss 🐲: 0.004480  [ 3232/ 9422]\n",
            "loss 🐲: 0.003399  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006796 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.019699  [   32/ 9422]\n",
            "loss 🐲: 0.003820  [ 3232/ 9422]\n",
            "loss 🐲: 0.003272  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006799 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.019341  [   32/ 9422]\n",
            "loss 🐲: 0.003912  [ 3232/ 9422]\n",
            "loss 🐲: 0.002910  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006511 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.017240  [   32/ 9422]\n",
            "loss 🐲: 0.004325  [ 3232/ 9422]\n",
            "loss 🐲: 0.003017  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006601 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.014864  [   32/ 9422]\n",
            "loss 🐲: 0.004066  [ 3232/ 9422]\n",
            "loss 🐲: 0.002868  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006763 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015531  [   32/ 9422]\n",
            "loss 🐲: 0.003907  [ 3232/ 9422]\n",
            "loss 🐲: 0.002983  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006646 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016764  [   32/ 9422]\n",
            "loss 🐲: 0.003644  [ 3232/ 9422]\n",
            "loss 🐲: 0.002484  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006516 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016476  [   32/ 9422]\n",
            "loss 🐲: 0.002852  [ 3232/ 9422]\n",
            "loss 🐲: 0.002610  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006493 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015820  [   32/ 9422]\n",
            "loss 🐲: 0.003827  [ 3232/ 9422]\n",
            "loss 🐲: 0.002793  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006420 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.017691  [   32/ 9422]\n",
            "loss 🐲: 0.003201  [ 3232/ 9422]\n",
            "loss 🐲: 0.002519  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006389 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.017027  [   32/ 9422]\n",
            "loss 🐲: 0.003408  [ 3232/ 9422]\n",
            "loss 🐲: 0.002576  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006511 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.017440  [   32/ 9422]\n",
            "loss 🐲: 0.003098  [ 3232/ 9422]\n",
            "loss 🐲: 0.002577  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006194 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016476  [   32/ 9422]\n",
            "loss 🐲: 0.002575  [ 3232/ 9422]\n",
            "loss 🐲: 0.002493  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006384 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.014547  [   32/ 9422]\n",
            "loss 🐲: 0.002874  [ 3232/ 9422]\n",
            "loss 🐲: 0.002249  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006340 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.014361  [   32/ 9422]\n",
            "loss 🐲: 0.002982  [ 3232/ 9422]\n",
            "loss 🐲: 0.002158  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006573 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.012233  [   32/ 9422]\n",
            "loss 🐲: 0.002814  [ 3232/ 9422]\n",
            "loss 🐲: 0.002235  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006273 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015217  [   32/ 9422]\n",
            "loss 🐲: 0.003041  [ 3232/ 9422]\n",
            "loss 🐲: 0.002101  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006490 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.012550  [   32/ 9422]\n",
            "loss 🐲: 0.003091  [ 3232/ 9422]\n",
            "loss 🐲: 0.002772  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006607 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016197  [   32/ 9422]\n",
            "loss 🐲: 0.002676  [ 3232/ 9422]\n",
            "loss 🐲: 0.002369  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006756 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016338  [   32/ 9422]\n",
            "loss 🐲: 0.002498  [ 3232/ 9422]\n",
            "loss 🐲: 0.002233  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006595 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.012957  [   32/ 9422]\n",
            "loss 🐲: 0.002396  [ 3232/ 9422]\n",
            "loss 🐲: 0.001880  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006726 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.013847  [   32/ 9422]\n",
            "loss 🐲: 0.002664  [ 3232/ 9422]\n",
            "loss 🐲: 0.001816  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006562 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.011334  [   32/ 9422]\n",
            "loss 🐲: 0.002503  [ 3232/ 9422]\n",
            "loss 🐲: 0.001841  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006878 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015933  [   32/ 9422]\n",
            "loss 🐲: 0.003013  [ 3232/ 9422]\n",
            "loss 🐲: 0.002242  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006901 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015795  [   32/ 9422]\n",
            "loss 🐲: 0.002605  [ 3232/ 9422]\n",
            "loss 🐲: 0.001710  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006802 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.015618  [   32/ 9422]\n",
            "loss 🐲: 0.002822  [ 3232/ 9422]\n",
            "loss 🐲: 0.001672  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006678 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.014129  [   32/ 9422]\n",
            "loss 🐲: 0.002746  [ 3232/ 9422]\n",
            "loss 🐲: 0.001785  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006576 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.016464  [   32/ 9422]\n",
            "loss 🐲: 0.002710  [ 3232/ 9422]\n",
            "loss 🐲: 0.002057  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006504 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.014492  [   32/ 9422]\n",
            "loss 🐲: 0.002284  [ 3232/ 9422]\n",
            "loss 🐲: 0.001784  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006631 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------🚀\n",
            "loss 🐲: 0.011079  [   32/ 9422]\n",
            "loss 🐲: 0.002306  [ 3232/ 9422]\n",
            "loss 🐲: 0.001823  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006355 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#Here we Wrap everything up and start the for loop to iterate through our two funcitions to do the training things, and custrunction is done here 👷🏼‍♀️ 🦸🏼‍♂️\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\\U0001F680\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "nyYvP-_MEBTg",
        "outputId": "68f1fa31-97bb-4de4-d5e5-ba0f69d6caff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx as onnx"
      ],
      "metadata": {
        "id": "wpvT8HhuD-WH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Dummy input for the model, which needs to match the shape of your actual input\n",
        "dummy_input = torch.randn(1, 144, 4).to(device)  # batch size = 1, sequence length = 144, input features = 4\n",
        "\n",
        "# Export the model\n",
        "onnx.export(model,                         # model to be exported\n",
        "            dummy_input,                   # an example input for tracing the model\n",
        "            \"model.onnx\",                  # where to save the ONNX file\n",
        "            export_params=True,            # store the trained weights inside the model\n",
        "            opset_version=12,              # ONNX version, 12 should be fine for most tasks\n",
        "            input_names=['input'],         # name of the input tensor\n",
        "            output_names=['output'],       # name of the output tensor\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # dynamic batch size\n",
        ")\n",
        "\n",
        "print(\"Model exported as ONNX\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4TFK-3HD3P_X",
        "outputId": "b316a2be-e36b-407a-cacf-c9fc679bcc38"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "Module onnx is not installed!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d80bed69e1ac>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Export the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m onnx.export(model,                         # model to be exported\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m                   \u001b[0;31m# an example input for tracing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# where to save the ONNX file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 )\n\u001b[1;32m   1721\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1723\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module onnx is not installed!\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aullJ9MXODmo"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Lv5fg8n6Tx9x"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}