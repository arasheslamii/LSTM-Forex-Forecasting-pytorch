{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2wE26o4NVMb",
        "outputId": "195efc0e-4999-4918-8203-dcbc39038511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# Libraries and Framework importation\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import sklearn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "end  = pd.to_datetime( datetime.now())\n",
        "start = end  - timedelta(days= 59)\n",
        "data = yf.download('USDJPY=X', start = start , end = end, interval= '5m' )\n",
        "df = torch.tensor(data.drop(columns=['Adj Close', 'Volume']).reset_index().drop(columns = ['Datetime']).to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y =  [], []\n",
        "for i in range(0,len(df)-144-60):\n",
        "  X.append(df[i:i+144])\n",
        "  y.append(df[i+144:(i+144+60)])"
      ],
      "metadata": {
        "id": "OfEIzwIhRXF1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating X_train, X_test, y_train, y_test\n",
        "\n",
        "train_size = 0.8\n",
        "X_train  = np.array(X[:int(train_size*len(X))])\n",
        "X_test  = np.array(X[int(train_size*len(X)):])\n",
        "y_train  = np.array(y[:int(train_size*len(y))])\n",
        "y_test  = np.array(y[int(train_size*len(y)):])\n",
        "\n",
        "#reshape to get ready for minmax scaling, because minmax scale got only 2D array\n",
        "\n",
        "X_train_reshape  = X_train.reshape(-1, X_train.shape[-1])\n",
        "X_test_reshape  = X_test.reshape(-1, X_test.shape[-1])\n",
        "y_train_reshape  = y_train.reshape(-1, y_train.shape[-1])\n",
        "y_test_reshape  = y_test.reshape(-1, y_train.shape[-1])\n",
        "\n",
        "#Scaling the data from #_###_reshape to\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "X_train_sc = scaler.fit_transform(X_train_reshape)\n",
        "y_train_sc = scaler.fit_transform(y_train_reshape)\n",
        "X_test_sc = scaler.transform(X_test_reshape)\n",
        "y_test_sc = scaler.transform(y_test_reshape )\n",
        "\n",
        "# Reshape the scaled data to getting back to their original shape\n",
        "X_train_sc = torch.from_numpy( X_train_sc.reshape(X_train.shape)).float()\n",
        "y_train_sc = torch.from_numpy(y_train_sc .reshape(y_train.shape)).float()\n",
        "X_test_sc = torch.from_numpy(X_test_sc .reshape(X_test.shape)).float()\n",
        "y_test_sc = torch.from_numpy(y_test_sc .reshape(y_test.shape)).float()"
      ],
      "metadata": {
        "id": "qkBKZaQiTCKc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'shape of the X_train: {X_train_sc.shape}, shpe of th X_test: {X_test.shape}, shpe of th y_train: {y_train.shape}, shpe of th y_test: {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eau0WoeCg9Dx",
        "outputId": "7581459a-3dae-4b05-9976-f4856fbaa258"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the X_train: torch.Size([9412, 144, 4]), shpe of th X_test: (2353, 144, 4), shpe of th y_train: (9412, 60, 4), shpe of th y_test: (2353, 60, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EDK0f9BYTf8F"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class custom_dataset(Dataset):\n",
        "  def __init__(self, X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  def  __len__(self):\n",
        "    return len(self.X)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataloader = DataLoader(custom_dataset(\n",
        "    X_train_sc, y_train_sc), batch_size = 32)\n",
        "\n",
        "test_dataloader = DataLoader(custom_dataset(\n",
        "    X_test_sc, y_test_sc), batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g_DuaNsabwh"
      },
      "source": [
        "Lets Creeat a custome Dataset to make a dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zQqWOtiwQJDZ"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size , hidden_size , num_layers, output_size , dropout_prob):\n",
        "    super(LSTM, self).__init__()\n",
        "    self.lstm  = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                         num_layers=num_layers, batch_first=True,\n",
        "                         dropout=dropout_prob)\n",
        "    self.fc1 = nn.Linear(hidden_size, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc_out = nn.Linear(64, output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "  def forward(self, X):  # This should be outside __init__\n",
        "    lstm_out, _ = self.lstm(X)\n",
        "    lstm_out = lstm_out[:, -1, :]  # Only keep the output from the last time step\n",
        "    out = self.relu(self.fc1(lstm_out))\n",
        "    out = self.dropout(out)\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc_out(out)\n",
        "    return out.view(out.size(0), 60, 4)\n",
        "\n",
        "input_size = 4  # Number of input features (Open, High, Low, Close)\n",
        "hidden_size = 128  # Number of LSTM neurons in each layer\n",
        "num_layers = 2  # Number of LSTM layers\n",
        "output_size = 240  # Predicting 60 future time steps, 4 features for each (60 * 4 = 240)\n",
        "dropout_prob = 0.3  # 30% dropout rate\n",
        "\n",
        "model = LSTM(input_size, hidden_size, num_layers, output_size, dropout_prob)\n",
        "\n",
        "# Initialize the loss function adn Optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "#Lets define our crucial things\n",
        "learning_rate = 1e-5\n",
        "batch_size = 32\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JxIMmZXjaDPy"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss = loss.item()\n",
        "      current = batch * batch_size +len(X)\n",
        "      print(f\"loss \\U0001F432: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Testing/Validation loop\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients during testing\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()  # Accumulate the loss\n",
        "\n",
        "    test_loss /= num_batches  # Average loss over all batches\n",
        "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "abPth-GO6qLv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS7xXxA5Nids",
        "outputId": "521a50d5-4b28-4c01-e8de-5a6f4c4756f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.689735  [   32/ 9422]\n",
            "loss ðŸ²: 0.200350  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.098525  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.440978 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.672932  [   32/ 9422]\n",
            "loss ðŸ²: 0.183608  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.071424  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.278003 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.438334  [   32/ 9422]\n",
            "loss ðŸ²: 0.086145  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.031896  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.167306 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.285459  [   32/ 9422]\n",
            "loss ðŸ²: 0.062696  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.019347  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.112722 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.201754  [   32/ 9422]\n",
            "loss ðŸ²: 0.044761  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.012041  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.078578 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.142762  [   32/ 9422]\n",
            "loss ðŸ²: 0.028818  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.008582  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.054974 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.102017  [   32/ 9422]\n",
            "loss ðŸ²: 0.018547  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.007284  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.039159 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.079676  [   32/ 9422]\n",
            "loss ðŸ²: 0.012694  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.006318  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.028541 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.062622  [   32/ 9422]\n",
            "loss ðŸ²: 0.009455  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.005957  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.021349 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.052980  [   32/ 9422]\n",
            "loss ðŸ²: 0.007983  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.005116  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.016616 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.041782  [   32/ 9422]\n",
            "loss ðŸ²: 0.007070  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.004942  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.013662 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.034757  [   32/ 9422]\n",
            "loss ðŸ²: 0.006560  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.004375  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.011610 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.036758  [   32/ 9422]\n",
            "loss ðŸ²: 0.005433  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.004172  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.009993 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.027986  [   32/ 9422]\n",
            "loss ðŸ²: 0.005570  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003769  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.009022 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.027796  [   32/ 9422]\n",
            "loss ðŸ²: 0.005659  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003590  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.008405 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.022589  [   32/ 9422]\n",
            "loss ðŸ²: 0.004714  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003674  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007657 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.023753  [   32/ 9422]\n",
            "loss ðŸ²: 0.004642  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003589  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007784 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.023998  [   32/ 9422]\n",
            "loss ðŸ²: 0.004801  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003466  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007152 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.021440  [   32/ 9422]\n",
            "loss ðŸ²: 0.005155  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003334  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.007002 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.023364  [   32/ 9422]\n",
            "loss ðŸ²: 0.004339  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003295  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006827 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.023954  [   32/ 9422]\n",
            "loss ðŸ²: 0.004480  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003399  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006796 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.019699  [   32/ 9422]\n",
            "loss ðŸ²: 0.003820  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003272  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006799 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.019341  [   32/ 9422]\n",
            "loss ðŸ²: 0.003912  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002910  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006511 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.017240  [   32/ 9422]\n",
            "loss ðŸ²: 0.004325  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.003017  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006601 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.014864  [   32/ 9422]\n",
            "loss ðŸ²: 0.004066  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002868  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006763 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015531  [   32/ 9422]\n",
            "loss ðŸ²: 0.003907  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002983  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006646 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016764  [   32/ 9422]\n",
            "loss ðŸ²: 0.003644  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002484  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006516 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016476  [   32/ 9422]\n",
            "loss ðŸ²: 0.002852  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002610  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006493 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015820  [   32/ 9422]\n",
            "loss ðŸ²: 0.003827  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002793  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006420 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.017691  [   32/ 9422]\n",
            "loss ðŸ²: 0.003201  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002519  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006389 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.017027  [   32/ 9422]\n",
            "loss ðŸ²: 0.003408  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002576  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006511 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.017440  [   32/ 9422]\n",
            "loss ðŸ²: 0.003098  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002577  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006194 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016476  [   32/ 9422]\n",
            "loss ðŸ²: 0.002575  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002493  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006384 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.014547  [   32/ 9422]\n",
            "loss ðŸ²: 0.002874  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002249  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006340 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.014361  [   32/ 9422]\n",
            "loss ðŸ²: 0.002982  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002158  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006573 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.012233  [   32/ 9422]\n",
            "loss ðŸ²: 0.002814  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002235  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006273 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015217  [   32/ 9422]\n",
            "loss ðŸ²: 0.003041  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002101  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006490 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.012550  [   32/ 9422]\n",
            "loss ðŸ²: 0.003091  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002772  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006607 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016197  [   32/ 9422]\n",
            "loss ðŸ²: 0.002676  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002369  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006756 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016338  [   32/ 9422]\n",
            "loss ðŸ²: 0.002498  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002233  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006595 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.012957  [   32/ 9422]\n",
            "loss ðŸ²: 0.002396  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001880  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006726 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.013847  [   32/ 9422]\n",
            "loss ðŸ²: 0.002664  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001816  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006562 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.011334  [   32/ 9422]\n",
            "loss ðŸ²: 0.002503  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001841  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006878 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015933  [   32/ 9422]\n",
            "loss ðŸ²: 0.003013  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002242  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006901 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015795  [   32/ 9422]\n",
            "loss ðŸ²: 0.002605  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001710  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006802 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.015618  [   32/ 9422]\n",
            "loss ðŸ²: 0.002822  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001672  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006678 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.014129  [   32/ 9422]\n",
            "loss ðŸ²: 0.002746  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001785  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006576 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.016464  [   32/ 9422]\n",
            "loss ðŸ²: 0.002710  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.002057  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006504 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.014492  [   32/ 9422]\n",
            "loss ðŸ²: 0.002284  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001784  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006631 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------ðŸš€\n",
            "loss ðŸ²: 0.011079  [   32/ 9422]\n",
            "loss ðŸ²: 0.002306  [ 3232/ 9422]\n",
            "loss ðŸ²: 0.001823  [ 6432/ 9422]\n",
            "Test Error: \n",
            " Avg loss: 0.006355 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#Here we Wrap everything up and start the for loop to iterate through our two funcitions to do the training things, and custrunction is done here ðŸ‘·ðŸ¼â€â™€ï¸ ðŸ¦¸ðŸ¼â€â™‚ï¸\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\\U0001F680\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "nyYvP-_MEBTg",
        "outputId": "68f1fa31-97bb-4de4-d5e5-ba0f69d6caff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.onnx as onnx"
      ],
      "metadata": {
        "id": "wpvT8HhuD-WH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Dummy input for the model, which needs to match the shape of your actual input\n",
        "dummy_input = torch.randn(1, 144, 4).to(device)  # batch size = 1, sequence length = 144, input features = 4\n",
        "\n",
        "# Export the model\n",
        "onnx.export(model,                         # model to be exported\n",
        "            dummy_input,                   # an example input for tracing the model\n",
        "            \"model.onnx\",                  # where to save the ONNX file\n",
        "            export_params=True,            # store the trained weights inside the model\n",
        "            opset_version=12,              # ONNX version, 12 should be fine for most tasks\n",
        "            input_names=['input'],         # name of the input tensor\n",
        "            output_names=['output'],       # name of the output tensor\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # dynamic batch size\n",
        ")\n",
        "\n",
        "print(\"Model exported as ONNX\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "4TFK-3HD3P_X",
        "outputId": "b316a2be-e36b-407a-cacf-c9fc679bcc38"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "Module onnx is not installed!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d80bed69e1ac>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Export the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m onnx.export(model,                         # model to be exported\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m                   \u001b[0;31m# an example input for tracing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# where to save the ONNX file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[0m\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 )\n\u001b[1;32m   1721\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1723\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module onnx is not installed!\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;31m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aullJ9MXODmo"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Lv5fg8n6Tx9x"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}